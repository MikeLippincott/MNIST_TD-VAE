{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Optuna for the Temporal Difference Variational Autoencoder (TD-VAE) model\n",
    "This notebook performs a hyperparameter optimization for the Temporal Difference Variational Autoencoder (TD-VAE) model using the Optuna library. \n",
    "The TD-VAE model is a variant of the VAE model that incorporates temporal difference learning to improve the quality of the learned representations. \n",
    "The hyperparameters that will be optimized include the learning rate, batch size, and the number of training epochs. \n",
    "The optimization will be performed using the Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lippincm/miniforge3/envs/tdvae_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import umap\n",
    "from matplotlib import gridspec\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "utils_path = pathlib.Path(\"../../utils/\").resolve(strict=True)\n",
    "sys.path.append(str(utils_path))\n",
    "\n",
    "from model import TD_VAE, DBlock, Decoder, PreProcess\n",
    "from prep_data import MNIST_Dataset\n",
    "from rollout import rollout_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "X_DIM = 784\n",
    "PROCESSED_X_DIM = 784\n",
    "OPTIM_EPOCHS = 5\n",
    "NUM_TRIALS = 250\n",
    "SAMPLES_PER_SEQ = 20\n",
    "TIME_CONSTANTS_MAX = 16  # There are 20 frames total\n",
    "TIME_JUMP_OPTIONS = [1, 2, 3, 4]  # Jump up to 4 frames away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to the MNIST images\n",
    "mnist_pickle_path = pathlib.Path(\"../../data/mnist/MNIST.pkl\").resolve(strict=True)\n",
    "# create the log directory if it does not exist\n",
    "log_path = pathlib.Path(\"../log/\").resolve()\n",
    "log_path.mkdir(exist_ok=True)\n",
    "log_file_path = pathlib.Path(\"../log/loginfo.txt\").resolve()\n",
    "# set and make the models directory\n",
    "models_path = pathlib.Path(\"../models/\").resolve()\n",
    "models_path.mkdir(exist_ok=True, parents=True)\n",
    "# set the path to the results directory\n",
    "results_path = pathlib.Path(\"../results/\").resolve()\n",
    "results_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "# make the log directory\n",
    "pathlib.Path(\"../log\").mkdir(exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=str(pathlib.Path(log_path / \"training_log.log\")), level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_image', 'train_label', 'test_image', 'test_label'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(mnist_pickle_path, \"rb\") as file_handle:\n",
    "    MNIST = pickle.load(file_handle)\n",
    "\n",
    "# get the MNIST data keys\n",
    "print(MNIST.keys())\n",
    "MNIST[\"train_image\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters search space\n",
    "BATCH_SIZE_MIN = 8\n",
    "BATCH_SIZE_MAX = 256\n",
    "BATCH_SIZE_STEP = 8\n",
    "\n",
    "LEARNING_RATE_MIN = 0.000001\n",
    "LEARNING_RATE_MAX = 0.001\n",
    "LEARNING_RATE_STEP = 0.0005\n",
    "\n",
    "BELIEF_STATE_SIZE_MIN = 5\n",
    "BELIEF_STATE_SIZE_MAX = 500\n",
    "\n",
    "STATE_SIZE_MIN = 2\n",
    "STATE_SIZE_MAX = 50\n",
    "\n",
    "D_BLOCK_HIDDEN_SIZE_MIN = 5\n",
    "D_BLOCK_HIDDEN_SIZE_MAX = 500\n",
    "\n",
    "DECODER_HIDDEN_SIZE_MIN = 5\n",
    "DECODER_HIDDEN_SIZE_MAX = 500\n",
    "\n",
    "OPTIMIZING_EPOCHS = 25\n",
    "OPTIMIZER_OPTIONS = [\"Adam\", \"SGD\", \"RMSprop\"]\n",
    "\n",
    "SGD_MOMENTUM_MIN = 0.01\n",
    "SGD_MOMENTUM_MAX = 0.99\n",
    "\n",
    "RMSprop_ALPHA_MIN = 0.01\n",
    "RMSprop_ALPHA_MAX = 0.99\n",
    "\n",
    "RMSprop_MOMENTUM_MIN = 0.01\n",
    "RMSprop_MOMENTUM_MAX = 0.99\n",
    "\n",
    "RMSprop_EPSILON_MIN = 0.0001\n",
    "RMSprop_EPSILON_MAX = 0.1\n",
    "\n",
    "N_LAYERS_MIN = 1\n",
    "N_LAYERS_MAX = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the objective function for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna objective\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter search\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial: optuna.Trial\n",
    "        The optuna trial object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The loss value for the model or the metric to be optimized\n",
    "    \"\"\"\n",
    "    # sample hyperparameters\n",
    "    batch_size = trial.suggest_int(\n",
    "        \"batch_size\", BATCH_SIZE_MIN, BATCH_SIZE_MAX, step=BATCH_SIZE_STEP\n",
    "    )\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", LEARNING_RATE_MIN, LEARNING_RATE_MAX\n",
    "    )\n",
    "    belief_state_size = trial.suggest_int(\n",
    "        \"belief_state_size\", BELIEF_STATE_SIZE_MIN, BELIEF_STATE_SIZE_MAX\n",
    "    )\n",
    "    state_size = trial.suggest_int(\"state_size\", STATE_SIZE_MIN, STATE_SIZE_MAX)\n",
    "    d_block_hidden_size = trial.suggest_int(\n",
    "        \"d_block_hidden_size\", D_BLOCK_HIDDEN_SIZE_MIN, D_BLOCK_HIDDEN_SIZE_MAX\n",
    "    )\n",
    "    decoder_hidden_size = trial.suggest_int(\n",
    "        \"decoder_hidden_size\", DECODER_HIDDEN_SIZE_MIN, DECODER_HIDDEN_SIZE_MAX\n",
    "    )\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", OPTIMIZER_OPTIONS)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", N_LAYERS_MIN, N_LAYERS_MAX)\n",
    "\n",
    "    # set up the model\n",
    "    model = TD_VAE(\n",
    "        x_size=X_DIM,\n",
    "        processed_x_size=PROCESSED_X_DIM,\n",
    "        b_size=belief_state_size,\n",
    "        z_size=state_size,\n",
    "        d_block_hidden_size=d_block_hidden_size,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        layers=n_layers,\n",
    "        samples_per_seq=SAMPLES_PER_SEQ,\n",
    "        t_diff_min=1,\n",
    "        t_diff_max=TIME_CONSTANTS_MAX,\n",
    "    )\n",
    "    # model.to_device()\n",
    "    model.cuda()\n",
    "\n",
    "    # set up the optimizer\n",
    "    if optimizer == \"SGD\":\n",
    "        momentum = trial.suggest_float(\"momentum\", SGD_MOMENTUM_MIN, SGD_MOMENTUM_MAX)\n",
    "        optimizer_kwargs = {\"momentum\": momentum}\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, **optimizer_kwargs)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        alpha = trial.suggest_float(\"alpha\", RMSprop_ALPHA_MIN, RMSprop_ALPHA_MAX)\n",
    "        momentum = trial.suggest_float(\n",
    "            \"momentum\", RMSprop_MOMENTUM_MIN, RMSprop_MOMENTUM_MAX\n",
    "        )\n",
    "        eps = trial.suggest_float(\"epsilon\", RMSprop_EPSILON_MIN, RMSprop_EPSILON_MAX)\n",
    "        optimizer_kwargs = {\"alpha\": alpha, \"momentum\": momentum, \"eps\": eps}\n",
    "        optimizer = optim.RMSprop(\n",
    "            model.parameters(), lr=learning_rate, **optimizer_kwargs\n",
    "        )\n",
    "    elif optimizer == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer\")\n",
    "\n",
    "    # set up the data loader\n",
    "    train_data = MNIST_Dataset(MNIST[\"train_image\"], MNIST[\"train_label\"])\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # train the model\n",
    "\n",
    "    for epoch in range(OPTIM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch, (idx, images) in enumerate(train_loader):\n",
    "            batch_counter = 0\n",
    "            batch_loss = 0\n",
    "            images = images[\"image\"].cuda()\n",
    "            # Make a forward step of preprocessing and LSTM\n",
    "            forward_return_tuple = model.forward(images)\n",
    "\n",
    "            # Randomly sample a time step and jumpy step\n",
    "            t_1 = np.random.choice(TIME_CONSTANTS_MAX)\n",
    "            t_2 = t_1 + np.random.choice(TIME_JUMP_OPTIONS)\n",
    "\n",
    "            # Calculate loss function based on two time points\n",
    "            loss, bce_diff, kl_div_qs_pb, kl_shift_qb_pt, bce_optimal = (\n",
    "                model.calculate_loss(forward_return_tuple)\n",
    "            )\n",
    "            if loss.isnan():\n",
    "                print(\"loss is nan\")\n",
    "                pass\n",
    "            elif loss.isinf():\n",
    "                print(\"loss is inf\")\n",
    "                pass\n",
    "            elif loss.item() == 0:\n",
    "                print(\"loss is zero\")\n",
    "                pass\n",
    "            elif loss.item() < 0:\n",
    "                print(\"loss is negative\")\n",
    "                pass\n",
    "            elif loss.item() > 0:\n",
    "                batch_counter += 1\n",
    "                batch_loss += loss.item()\n",
    "                # must clear out stored gradient\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        epoch_loss += batch_loss / batch_counter\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-16 08:43:48,389] Using an existing study with name 'TD_VAE_hyperparameter_search' instead of creating a new one.\n",
      "[I 2024-08-16 08:46:27,363] Trial 4 finished with value: 199.5407257080078 and parameters: {'batch_size': 144, 'learning_rate': 0.0007154741770060472, 'belief_state_size': 303, 'state_size': 28, 'd_block_hidden_size': 215, 'decoder_hidden_size': 325, 'optimizer': 'RMSprop', 'n_layers': 6, 'alpha': 0.7858905373210113, 'momentum': 0.5283170213578464, 'epsilon': 0.05684765165328384}. Best is trial 3 with value: 139.24745178222656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 144, 'learning_rate': 0.0007154741770060472, 'belief_state_size': 303, 'state_size': 28, 'd_block_hidden_size': 215, 'decoder_hidden_size': 325, 'optimizer': 'RMSprop', 'n_layers': 6, 'alpha': 0.7858905373210113, 'momentum': 0.5283170213578464, 'epsilon': 0.05684765165328384, 'best_loss': 139.24745178222656}\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_search_path = pathlib.Path(\"../hyperparameter_search\").resolve()\n",
    "hyperparameter_search_path.mkdir(exist_ok=True, parents=True)\n",
    "# set up the optuna study\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"TD_VAE_hyperparameter_search\",\n",
    "    storage=\"sqlite:///../hyperparameter_search/td_vae_hyperparameter_search.db\",\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(seed=0),\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=NUM_TRIALS)\n",
    "\n",
    "# get best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_params[\"best_loss\"] = study.best_value\n",
    "print(best_params)\n",
    "\n",
    "# save the best hyperparameters and the best model\n",
    "best_params_path = pathlib.Path(models_path / \"best_params.json\")\n",
    "with open(best_params_path, \"w\") as file_handle:\n",
    "    json.dump(best_params, file_handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdvae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
